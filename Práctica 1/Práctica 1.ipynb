{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbcf848",
   "metadata": {},
   "source": [
    "# Práctica 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3416a",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9693568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import month, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e65db3",
   "metadata": {},
   "source": [
    "## Inicio de sesión en Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d51337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicio de la sesión de Spark.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MarathonRegression\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9484a8",
   "metadata": {},
   "source": [
    "## Visualización inicial de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa47690",
   "metadata": {},
   "source": [
    "### Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702073d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset.\n",
    "df = spark.read.csv(r\"C:\\Users\\Usuario\\Desktop\\ALEX\\GCED\\7º cuatri\\aprendizaje automático a gran escala\\run_ww_2020_d.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53273c03",
   "metadata": {},
   "source": [
    "### Muestra de las cinco primeras filas y del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b18ae0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+--------+------------------+------+---------+--------------+-----------------------+\n",
      "|_c0|datetime  |athlete|distance|duration          |gender|age_group|country       |major                  |\n",
      "+---+----------+-------+--------+------------------+------+---------+--------------+-----------------------+\n",
      "|0  |2020-01-01|0      |0.0     |0.0               |F     |18 - 34  |United States |CHICAGO 2019           |\n",
      "|1  |2020-01-01|1      |5.72    |31.633333333333333|M     |35 - 54  |Germany       |BERLIN 2016            |\n",
      "|2  |2020-01-01|2      |0.0     |0.0               |M     |35 - 54  |United Kingdom|LONDON 2018,LONDON 2019|\n",
      "|3  |2020-01-01|3      |0.0     |0.0               |M     |18 - 34  |United Kingdom|LONDON 2017            |\n",
      "|4  |2020-01-01|4      |8.07    |38.61666666666667 |M     |35 - 54  |United States |BOSTON 2017            |\n",
      "+---+----------+-------+--------+------------------+------+---------+--------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- datetime: date (nullable = true)\n",
      " |-- athlete: integer (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age_group: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- major: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las cinco primeras filas.\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Mostrar esquema de columnas.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133fb33",
   "metadata": {},
   "source": [
    "### Muestra de filas y variables y principales estadísticas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e180237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 13326792, Variables: 9\n",
      "+-------+-----------------+------------------+\n",
      "|summary|         distance|          duration|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|         13326792|          13326792|\n",
      "|   mean|3.864718473881684| 21.39197543564929|\n",
      "| stddev|6.661547347662435| 39.27358918572176|\n",
      "|    min|              0.0|               0.0|\n",
      "|    max|           347.95|2299.9666666666667|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "+------+--------+\n",
      "|gender|   count|\n",
      "+------+--------+\n",
      "|     F| 3253374|\n",
      "|     M|10073418|\n",
      "+------+--------+\n",
      "\n",
      "+---------+-------+\n",
      "|age_group|  count|\n",
      "+---------+-------+\n",
      "|     55 +| 940254|\n",
      "|  35 - 54|7905966|\n",
      "|  18 - 34|4480572|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Número de filas y variables.\n",
    "print(f\"Filas: {df.count()}, Variables: {len(df.columns)}\")\n",
    "\n",
    "# Estadísticas descriptivas de variables numéricas.\n",
    "df.describe(['distance', 'duration']).show()\n",
    "\n",
    "# Distribución por género y grupo de edad.\n",
    "df.groupBy(\"gender\").count().show()\n",
    "df.groupBy(\"age_group\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e85d52",
   "metadata": {},
   "source": [
    "## Preparación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66c5c0",
   "metadata": {},
   "source": [
    "### Creación de la variable \"season\" a partir de la datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e502a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"month\", month(\"datetime\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"season\",\n",
    "    when((df.month >= 3) & (df.month <= 5), \"spring\")\n",
    "    .when((df.month >= 6) & (df.month <= 8), \"summer\")\n",
    "    .when((df.month >= 9) & (df.month <= 11), \"autumn\")\n",
    "    .otherwise(\"winter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb392c8",
   "metadata": {},
   "source": [
    "### Filtrado de filas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3af6f",
   "metadata": {},
   "source": [
    "Como tenemos un gran número de filas, vamos a filtrar. Primero, nos quedamos solo con los registros que no estén vacíos, es decir, aquellos cuya duración y distancia sea mayor a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bef4539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas tras filtrar: 4581764\n"
     ]
    }
   ],
   "source": [
    "# Filtrado de filas.\n",
    "df_filtered = df.filter((df.duration > 0) & (df.distance > 0))\n",
    "\n",
    "# Comprobar tamaño.\n",
    "print(f\"Filas tras filtrar: {df_filtered.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266323a4",
   "metadata": {},
   "source": [
    "Como seguimos teniendo gran número de filas, volvemos a filtrar. Ahora vamos a filtrar por número de atletas. Inicialmente tenemos 36.7k atletas. Vamos a probar con cuantos nos quedamos con el número de filas más adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a000e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas con athlete < 5000: 629313\n",
      "Filas con athlete < 8000: 1001656\n",
      "Filas con athlete < 10000: 1255519\n",
      "Filas con athlete < 15000: 1878765\n",
      "Filas con athlete < 20000: 2483092\n"
     ]
    }
   ],
   "source": [
    "# Filtrar atletas con ID < 5000.\n",
    "df_filtered_5000 = df_filtered.filter(df_filtered.athlete < 5000)\n",
    "print(f\"Filas con athlete < 5000: {df_filtered_5000.count()}\")\n",
    "\n",
    "# Filtrar atletas con ID < 8000.\n",
    "df_filtered_8000 = df_filtered.filter(df_filtered.athlete < 8000)\n",
    "print(f\"Filas con athlete < 8000: {df_filtered_8000.count()}\")\n",
    "\n",
    "# Filtrar atletas con ID < 10000.\n",
    "df_filtered_10000 = df_filtered.filter(df_filtered.athlete < 10000)\n",
    "print(f\"Filas con athlete < 10000: {df_filtered_10000.count()}\")\n",
    "\n",
    "# Filtrar atletas con ID < 15000.\n",
    "df_final = df_filtered.filter(df_filtered.athlete < 15000)\n",
    "print(f\"Filas con athlete < 15000: {df_final.count()}\")\n",
    "\n",
    "# Filtrar atletas con ID < 20000.\n",
    "df_filtered_20000 = df_filtered.filter(df_filtered.athlete < 20000)\n",
    "print(f\"Filas con athlete < 20000: {df_filtered_20000.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a754fa",
   "metadata": {},
   "source": [
    "Nos quedamos con 15000 atletas (1878765 filas), ya que creemos que es el valor más adecuado para lograr un equilibrio entre cantidad para un correcto aprendizaje y velocidad de procesamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccf50a",
   "metadata": {},
   "source": [
    "### Muestra de filas y variables y principales estadísticas del dataset filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02328486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 1878765, Variables: 11\n",
      "+-------+------------------+--------------------+\n",
      "|summary|          distance|            duration|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|           1878765|             1878765|\n",
      "|   mean|11.337875722616882|    62.6373797680926|\n",
      "| stddev|  6.85062966629291|  44.442787948108126|\n",
      "|    min|              0.01|0.016666666666666666|\n",
      "|    max|            263.37|              2202.0|\n",
      "+-------+------------------+--------------------+\n",
      "\n",
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     F| 466030|\n",
      "|     M|1412735|\n",
      "+------+-------+\n",
      "\n",
      "+---------+-------+\n",
      "|age_group|  count|\n",
      "+---------+-------+\n",
      "|     55 +| 138997|\n",
      "|  35 - 54|1123696|\n",
      "|  18 - 34| 616072|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Número de filas y variables.\n",
    "print(f\"Filas: {df_final.count()}, Variables: {len(df_final.columns)}\")\n",
    "\n",
    "# Estadísticas descriptivas de variables numéricas.\n",
    "df_final.describe(['distance', 'duration']).show()\n",
    "\n",
    "# Distribución por género y grupo de edad.\n",
    "df_final.groupBy(\"gender\").count().show()\n",
    "df_final.groupBy(\"age_group\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d6f8e",
   "metadata": {},
   "source": [
    "### Seleccionamos solo las columnas relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af157dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las columnas que no son datetime o major.\n",
    "df_final = df_final.select(\"athlete\", \"distance\", \"duration\", \"gender\", \"age_group\", \"country\", \"season\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77d0b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "276297fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.dropna(subset=[\"athlete\", \"distance\", \"duration\", \"gender\", \"age_group\", \"country\", \"season\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d7c1a",
   "metadata": {},
   "source": [
    "## Preparación de los datos para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312546a",
   "metadata": {},
   "source": [
    "### Convertimos los datos categóricos en vectores one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af946dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las columnas de tipo categórico en índices numéricos. \n",
    "gender_indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\")\n",
    "age_indexer = StringIndexer(inputCol=\"age_group\", outputCol=\"age_index\")\n",
    "country_indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_index\")\n",
    "season_indexer = StringIndexer(inputCol=\"season\", outputCol=\"season_index\")\n",
    "\n",
    "# Convertimos los índices numéricos del paso anterior en vectores one-hot.\n",
    "encoder = OneHotEncoder(inputCols=[\"gender_index\", \"age_index\", \"country_index\", \"season_index\"],\n",
    "                        outputCols=[\"gender_vec\", \"age_vec\", \"country_vec\", \"season_vec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9fcb1",
   "metadata": {},
   "source": [
    "### Combinamos las columnas numéricas en un solo vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaf712f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un vector features que combine todas las variables numéricas.\n",
    "assembler = VectorAssembler(inputCols=[\"distance\", \"gender_vec\", \"age_vec\", \"country_vec\", \"season_vec\"],\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e8ad9",
   "metadata": {},
   "source": [
    "### Creamos un pipeline para aplicar todas las transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8880510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el Pipeline con todas las etapas.\n",
    "pipeline = Pipeline(stages=[gender_indexer, age_indexer, country_indexer, season_indexer, encoder, assembler])\n",
    "\n",
    "# Ajustamos el pipeline y transformamos los datos.\n",
    "df_prepared = pipeline.fit(df_final).transform(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6703fd2",
   "metadata": {},
   "source": [
    "### División de los datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los atletas.\n",
    "athletes = df_final.select(\"athlete\").distinct()\n",
    "\n",
    "# Asignamos aleatoriamente el 80% de atletas a train y el 20% a test.\n",
    "train_athletes, test_athletes = athletes.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Filtramos filas según atletas asignados.\n",
    "train_df = df_prepared.join(train_athletes, on=\"athlete\", how=\"inner\")\n",
    "test_df = df_prepared.join(test_athletes, on=\"athlete\", how=\"inner\")\n",
    "\n",
    "# Mostramos el número de filas en train y test.\n",
    "print(f\"Train: {train_df.count()} filas, Test: {test_df.count()} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aa302",
   "metadata": {},
   "source": [
    "## Creación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb6078",
   "metadata": {},
   "source": [
    "### Modelo Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbba4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------+\n",
      "|features                                   |label             |\n",
      "+-------------------------------------------+------------------+\n",
      "|(113,[0,1,2,6,112],[5.72,1.0,1.0,1.0,1.0]) |31.633333333333333|\n",
      "|(113,[0,1,2,4,112],[8.07,1.0,1.0,1.0,1.0]) |38.61666666666667 |\n",
      "|(113,[0,2,4,112],[10.09,1.0,1.0,1.0])      |43.56666666666667 |\n",
      "|(113,[0,1,4,112],[9.82,1.0,1.0,1.0])       |50.53333333333333 |\n",
      "|(113,[0,1,2,5,112],[10.05,1.0,1.0,1.0,1.0])|59.05             |\n",
      "+-------------------------------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Definir la columna objetivo\n",
    "df_model = df_prepared.withColumnRenamed(\"duration\", \"label\")\n",
    "\n",
    "# Comprobamos\n",
    "df_model.select(\"features\", \"label\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14942504",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o575.fit.\n: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] User defined function (`StringIndexerModel$$Lambda$5161/0x00000114112b2a58`: (string) => double) failed due to: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.. SQLSTATE: 39000\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:195)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:266)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd$lzycompute(Dataset.scala:1578)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd(Dataset.scala:1576)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$rdd$1(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\r\n\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:64)\r\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:330)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:328)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:185)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:195)\r\n\t\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\t\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t\t... 1 more\r\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:377)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:372)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m lr = LinearRegression(featuresCol=\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m, labelCol=\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m lr_model = \u001b[43mlr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Generar predicciones sobre el conjunto de test\u001b[39;00m\n\u001b[32m     15\u001b[39m lr_predictions = lr_model.transform(test_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\aage\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\aage\\Lib\\site-packages\\pyspark\\ml\\util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\aage\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\aage\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\aage\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\aage\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\aage\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o575.fit.\n: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] User defined function (`StringIndexerModel$$Lambda$5161/0x00000114112b2a58`: (string) => double) failed due to: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.. SQLSTATE: 39000\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:195)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:266)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd$lzycompute(Dataset.scala:1578)\r\n\tat org.apache.spark.sql.classic.Dataset.materializedRdd(Dataset.scala:1576)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$rdd$1(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd$lzycompute(Dataset.scala:1586)\r\n\tat org.apache.spark.sql.classic.Dataset.rdd(Dataset.scala:1584)\r\n\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:64)\r\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:330)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:328)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:185)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:195)\r\n\t\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\t\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t\t... 1 more\r\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:377)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:372)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Asegurarnos de que la columna 'label' existe\n",
    "train_df = train_df.withColumnRenamed(\"duration\", \"label\")\n",
    "test_df = test_df.withColumnRenamed(\"duration\", \"label\")\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Generar predicciones sobre el conjunto de test\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluador para regresión\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calcular métricas\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(lr_predictions)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(lr_predictions)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"=== Linear Regression ===\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# (Opcional) Mostrar algunas predicciones reales vs. predichas\n",
    "lr_predictions.select(\"athlete\", \"label\", \"prediction\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d936b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
